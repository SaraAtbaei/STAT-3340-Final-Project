```{r}
library(readr)
Real_estate <- read_csv("~/Desktop/Real estate.csv")
```
## (Data Description) pairs plot
```{r}
pairs(Real_estate[,1:6], pch = 414)
install.packages("olsrr")
library(olsrr)
plot(Real_estate[, -1])
```

##(Data Description) Deleted Studentized Residuals vs Fitted Values Plot
```{r}
model<-lm(Y.house.price.of.unit.area~X1.transaction.date+X2.house.age+X3.distance.to.the.nearest.MRT.station+X4.number.of.convenience .stores+X5.latitude+X6.longitude,data=Real_estate)
Pairs plot:
ols_plot_resid_stud_fit(model)
```
graph for our outliers

##(Data Description) outlier points
```{r}
cooks.distance(model)
```
No:36,48,106,114,117,127,129,149,165,167,221,229,271,313,345,362,383,387,390 are outliers because they're greater than 0.01 (threshold)

#(Data Description) cooks D bar model
```{r}
ols_plot_cooksd_bar(model)
```
For added data point we will take the average of outliers in Excel which is added as row 415

Citation:
@Manual {,  title        = { Measures of Influence},
  author       = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address      = {Vienna, Austria},
  year         =  2020,December.6th,
  url          = {https://www.R-project.org}{ https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html}
}

### Methods stepwise regression
```{r}
library("leaps")
data=read_csv("~/Desktop/Real estate.csv")
data=as.matrix(data[,-1])
leaps(x=data[,-1],y=data[,1],method="adjr2")
```
Largest R^2 adjusted is [17]=>0.0311625915
According to the adjusted R^2 Criterion, the model chosen is the one with the largest adjusted R^2,
in this case, this is the model with second Row of 4> T T F F F T
This is the model X1,X2,X6 

```{r}
y=data[,7]; x1=data[,1];x2=data[,2];x3=data[,3];x4=data[,4];x5=data[,5];x6=data[,6]
newmodel=lm(y~x1+x2+x4+x5+x6)
esresids=rstudent(newmodel)
par(mfrow=c(2,1))
qqnorm(esresids,main="normal QQ plot of externally standardized residuals")
plot(fitted(newmodel),esresids,xlab="fitted values",ylab="ext stand resids",main="externally standardized residuals vs fitted values")
```


### Methods, stepwise forward direction
```{r}
nullmodel=lm(y~1)
fullmodel=(lm(y~x1+x2+x3+x4+x5+x6))
step1=step(nullmodel,scope=list(lower=nullmodel,upper=fullmodel),direction="forward")
summary(step1)
```
we get: model y ~ x3 + x4 + x2 + x5 + x1

##MEthods, Stepwise backward direction
```{r}
fullmodel2=lm(y~x1*x2*x1*x3*x1*x4*x1*x5*x1*x6*x2*x3*x2*x4*x2*x5*x2*x6*x3*x4*x3*x5*x3*x6*x4*x5*x4*x6*x5*x6)
step2=step(fullmodel2,direction="backward")
summary(step2)
anova(step1,step2)
```
we get
  Model 1: y ~ x3 + x4 + x2 + x5 + x1
 Model 2: y ~ x1 + x2 + x3 + x4 + x5 + x6 + x1:x3 + x2:x3 + x1:x4 + x2:x4 + 
  x3:x4 + x2:x5 + x3:x5 + x4:x5 + x2:x6 + x3:x6 + x4:x6 + x1:x3:x4 + 
  x2:x3:x4 + x2:x3:x5 + x2:x4:x5 + x2:x3:x6 + x2:x4:x6 + x3:x4:x6 + 
  x2:x3:x4:x6
 
##Methods, Stepwise "both" direction
```{r}
step(nullmodel,scope=list(lower=nullmodel,upper=fullmodel2),direction="both")
```
We get: lm(formula = y ~ x3 + x4 + x2 + x5 + x1 + x3:x4 + x3:x5 + x4:x5 + 
     x3:x4:x5)
     
##VIF
```{r}
Backwardmodel2=lm(formula = y ~ x3 + x4 + x2 + x5 + x1 + x3:x4 + x3:x5 + x4:x5 + 
                   x3:x4:x5)
Backwardmodel1=lm(formula=y ~ x3 + x4 + x2 + x5 + x1)
library(car)
vif(Backwardmodel1)
vif(Backwardmodel2)
```
    
     
### Method, Backward direction II
```{r}
y=data[,7]; x1=data[,1];x2=data[,2];x3=data[,3];x4=data[,4];x5=data[,5];x6=data[,6]
model=lm(y~x1+x2+x3+x4+x5+x6)
summary(model)
anova(model)
```
Here, we get the final model y~x1+x2+x3+x4+x5


### Test for Gauss-Markov Assumption 
```{r}
y=data[,7]; x1=data[,1];x2=data[,2];x3=data[,3];x4=data[,4];x5=data[,5];x6=data[,6]
newmodel=lm(y~x1+x2+x3+x4+x5)
plot(newmodel)
```



